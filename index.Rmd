---
title: "Practical Machine Learning Final"
author: "David Bread"
date: "April 4, 2018"
output: html_document
---

```{r setup, echo = FALSE, eval=TRUE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)

library(dplyr)
library(arm)
library(ISLR)
library(caret)
library(ggplot2)
library(parallel)
library(doParallel)
library(corrplot)

rt <- "k:/Machine Learning/"
setwd(rt)

train <- "pml-training.csv"
train_load <- read.csv(train, header = TRUE, sep = ",")
test <- "pml-testing.csv"
test_load <- read.csv(test, header = TRUE, sep = ",")

#Update file for "new window" records
prep <- function(d) {
  nw <- filter(d, new_window == "yes")
  ow <- filter(d, new_window == "no")
  ow <- ow[,!apply(ow, 2, function(x) all(gsub(" ", "", x)=="", na.rm=TRUE))]
  ow <- ow[,-c(1:7)] 
  on <- colnames(ow)
  nw <- subset(nw, select=c(on))
  ret <- rbind(nw, ow)
}

tr1 <- prep(train_load)
ts1 <- prep(test_load)

```

## Project Approach 
In this project, we've been asked to use machine learning to predict the *classe* value in a set of observations capturing how a weight-lifting exercise was done. It is a supervised problem in that all the predictor variables, and the outcome variable, are labeled. We've been given two files--one with the training data and the other with the test data. So the problem does not involve partitioning the data into training and test sets. We'll structure our approach to finding a solution as follows:

1) Evaluate whether we need all the features present in the training data, and transform as necessary. Make the same changes in the test data. 
2) Use the "train" function in *caret* to build separate models using a variety of different machine learning algorithms, holding the random seed and tuning parameters constant to make replication and comparison possible 
3) Compare the accuracy of the algorithms and their out-of-sample error rates
4) Use the best algorithm to predict the classe value of each observation in the test dataset; apply the predictions to answer the 20 questions in the Week 4 Prediction quiz

Per Len Greski on his github page, the key to getting 100% accuracy on the prediction quiz to achieve 99% plus accuracy in our final model. He also mentioned that models that achieve this level of accuracy might also be computationally intensive, and seriously tax the capabilities of many home computers. This [link](https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md) describes how to harness parallel processing to boost PC performance in order to run modeling jobs in a reasonable amount of time, an approach I employed in this project. 

## Dataset Transformation
The original training dataset has 160 variables and 19622 observations. I removed the first six columns of purely descriptive information (row index, user, timestamp columns, "new window", and "num window" columns). Apparently, there are 107 columns that are exclusively associated with the "new window" function (i.e. the "new_window" column is equal to "yes"), representing a total 406 observations--0.02% of all the records in the training dataset. For the remaining 19216 records in the dataset, the values in these 107 columns are null or a literal "NA". Therefore, I removed any columns containing data specific to this function, as this data was not available to the remaining 19216 records (I felt it would be overkill to attempt to impute the data in these columns as they represent such a small percentage of the total number of records). After culling, 53 columns remain for the full training set of 19622 observations, the final column being the outcome variable "classe". I repeated this procedure on the testing dataset.

Fifty two predictors is still alot. We could check if any of these variables are highly correlated, or at or close to zero, and transform them accordingly. Alernatively, we could could pre-process the training dataset (using Principal Components Analysis) which should reduce the number of predictors automatically. Let's look at correlations first using *corrpot* package.

```{r corr, eval = TRUE, cache = TRUE, echo = FALSE}

M <- abs(cor(tr1[,1:52]))
corrplot(M, method = "circle", tl.cex = 0.5) #plot matrix

```

As you can see from the matrix, there are few correlations and they all are positive (blue). (Negative correlations are displayed in red.) Circle shading and size are used to represent the degree of correlation. In the interests of getting a model with lower bias, we'll opt for greater complexity and leave in all the 52 predictors.

## Training the Models
*Classe* is a factor variable taking five values--A, B, C, D, E--each representing a particular way in which a weight-lifting exercise was done. Predicting the classe variable in the 20 observations in the test dataset is an example of a non-binary classification problem. 

Machine learning algorithms generally fall into regression models (e.g. lm), and non-linear models (e.g. random forest). *caret* standardizes the interface to these different models using the *train* function. The Practical Machine Learning course provided not much than general guidance for selecting a machine learning algorithm for a given problem. My final choices of algos to try reflect those discussed in the course, but there are literally hundreds of different machine learning alorithms one could use to train a model using the caret package, as this [link](https://rdrr.io/cran/caret/man/models.html) abundantly demonstrates. The following table summarizes the models I used in the order in which they were trained.


Regression                            | Non-Linear
--------------------------------------|-----------------
1. Generalized Linear Model (GLM)     | 3. Classification and Regression Trees (CART)
2. Linear Discriminant Analysis (LDA) | 4. Support Vector Machines (SVM)
.                                     | 5. K-Nearest Neighbors (KNN)
.                                     | 6. Random Forest (RF)

Hopefully, at least one model in this set, properly parameterized, would be sufficient to achieve the predication goal of 99% plus accuracy.

Per Len's instruction, I enabled multi-core processing to boost performance, and used the same seed value (4939) to ensure that any randomized functions of the machine learning algrorithms were replicable. The *control* parameter of train also needed to be consistent to ensure that the resulting accuracy measures were comparable. The control parameter is shown below under *Model Comparison*.

I then fit models on each training set, and saved the resulting models using the saveRDS command to avoid having to re-compute them. Finally, I predicted the classe variable in each of the 20 test set observations using each of the models. 

```{r train_models, eval=TRUE, echo=FALSE, cache=TRUE}

#turn on parallel processing to reduce time of model creation
#cluster <- makeCluster(detectCores() - 1)
#registerDoParallel(cluster)
control <- trainControl(method = "repeatedcv", number = 10, repeats = 3, allowParallel = TRUE)

#linear models

# glm
set.seed(4939)
fit.glm <- train(classe~., data=tr1, method="bayesglm", trControl=control)
#saveRDS(fit.glm, "fit.glm.rds")

# LDA
#set.seed(4939)
#fit.lda <- train(classe~., data=tr1, method="lda", trControl=control)
#saveRDS(fit.lda, "fit.lda.rds")



#non-linear models

# CART
#set.seed(4939)
#fit.cart <- train(classe~., data=tr1, method="rpart", trControl=control)
#saveRDS(fit.cart, "fit.cart.rds")

# SVM
#set.seed(4939)
#fit.svm <- train(classe~., data=tr1, method="svmRadial", trControl=control)
#saveRDS(fit.svm, "fit.svm.rds")

# KNN
#set.seed(4939)
#fit.knn <- train(classe~., data=tr1, method="knn", trControl=control)
#saveRDS(fit.knn, "fit.knn.rds")

# Random Forest
#set.seed(4939)
#fit.rf <- train(classe~., data=tr1, method="rf", trControl=control)
#saveRDS(fit.rf, "fit.rf.rds")

#stopCluster(cluster)

```


```{r predict, echo=FALSE, cache=TRUE, eval = TRUE}

#load from rds file
#fit.glm <- readRDS("fit.glm.rds")
fit.cart <- readRDS("fit.cart.rds")
fit.lda <- readRDS("fit.lda.rds")
fit.svm <- readRDS("fit.svm.rds")
fit.knn <- readRDS("fit.knn.rds")
fit.rf <- readRDS("fit.rf.rds")

#predict from models
fit.glm.p <- predict(fit.glm, tr1)
fit.cart.p <- predict(fit.cart, tr1)
fit.lda.p <- predict(fit.lda, tr1)
fit.svm.p <- predict(fit.svm, tr1)
fit.knn.p <- predict(fit.knn, tr1)
fit.rf.p <- predict(fit.rf, tr1)

```

## Model Comparison

Before diving into model comparison, 

Out-of-sample error is estimated in regression algorithms by "mean squared error (MSE)" or "root mean squared error (RMSE)", and in non-regression algorithms by "cross-validation". Since the model that ulimately solved the program was non-linear, we'll look at cross-validation is some detail. In cross-validation, we utilize a portion of the training dataset for testing purposes. There are many variants on this. Since our final model uses k-fold cross-validation, I'll use this to explain. 

Let's look at the trainControl parameters that I'll hold constant while training each model. 


``` {r eval = FALSE, echo = TRUE}
control <- trainControl(method = "repeatedcv", number = 10, repeats = 3, allowParallel = TRUE)

```

Using repeatedcv with 10 folds, repeated 3 times, we are essentially breaking the training dataset up into 10 training sets of equal size, and holding out one of the sets to use for out-of-sample error estimation. So we create model using on the 9 sets and test on 1, and thereby obtain an accuracy estimate of the fold while testing. 

We then interate through each of the 10 folds in this manner, holding out a different set for testing at each iteration. At each iteration, we obtain a model trained on that fold. Then we test the model with the held out test data for that fold. 

The results obtained when the model was trained is compared with the results of the test. The delta of the correct classifications versus incorrect classifications is tabulated. This value represents the accuracy of the model for a given iteration. Out-of-sample error is 1 - accuracy rate. 

Since we specified "repeat = 3" as another trainControl parameter, we repeat the entire process again twice. The final result represents an aggregate of all three runs of the algorithm. 

We then 

In regression alorithms, out-of-sample error is estimated by calculating the RMSE. In non-linear algorithms, out-of-sample error is estimated by running the test data held out in each fold through the model trained on that fold.
Iterating through subsequent folds  -- need more info

The confusion matrix will help us compare the results of our models. 

``` {r confuse, eval = TRUE, echo = FALSE, cache = TRUE}

#confusionMatrix(actual from training data cross-validation, predicted)
c1 <- confusionMatrix(tr1$classe, fit.glm.p)
c2 <- confusionMatrix(tr1$classe, fit.cart.p)
c3 <- confusionMatrix(tr1$classe, fit.lda.p)
c4 <- confusionMatrix(tr1$classe, fit.svm.p)
c5 <- confusionMatrix(tr1$classe, fit.knn.p)
c6 <- confusionMatrix(tr1$classe, fit.rf.p)

knitr::kable(list("GLM Model______", " CART Model______", "LDA Model"))
knitr::kable(list(c1$table, "_______", c2$table, "_______", c3$table))
knitr::kable(list("SVM Model______", " KNN Model______", "RF Model"))
knitr::kable(list(c4$table,"_______", c5$table, "_______", c6$table))

```


## And the Winner Is...Random Forest!

With well over 99% accuracy, this was the algo that predicted the correct Classe values for all 20 test observations. 
We could "resample"" to aggregate the results of all these runs, but it may not improve anything.  

It's not clear that random forest would be a suitable solution in many situations where latency is an issue. This algo took by far the longest to run (almost 25 minutes), even with parallel processing enabled. Luckily, this project is not a real-time application, and we can afford to go with the solution with the highest accuracy.


```{r finish, echo=FALSE, cache=TRUE, eval=TRUE}

#make predictions from test dataset
p.classe  <- predict(fit.rf, ts1)
print(p.classe)

```

